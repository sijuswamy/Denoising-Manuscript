<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Siju K S">
<meta name="author" content="Dr.&nbsp;Vipin V.">
<meta name="dcterms.date" content="2024-09-17">
<meta name="keywords" content="Bilevel optimization, Denoising, Hyper-parameter tuning, Additive noise, Gaussian smoothing, Denoising algorithms, Mean squared error (MSE), Peak signal-to-noise ratio (PSNR), Gradient descent, Finite difference method, Optimization">

<title>Automatic Turning of Denoising Algorithms Parameters Without Ground Truth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Automatic Turning of Denoising Algorithms Parameters Without Ground Truth">
<meta name="citation_abstract" content="This review explores the mathematical concepts, noise types, and denoising algorithms, focusing on additive Gaussian noise. The paper&amp;amp;#039;s methodology was reproduced using Gaussian smoothing, with MSE and PSNR metrics. Optimization of the error function was performed using various techniques, including gradient descent and `scipy` minimization, highlighting the performance of simple denoisers.
">
<meta name="citation_keywords" content="Bilevel optimization,Denoising,Hyper-parameter tuning,Additive noise,Gaussian smoothing,Denoising algorithms,Mean squared error (MSE),Peak signal-to-noise ratio (PSNR),Gradient descent,Finite difference method,Optimization">
<meta name="citation_author" content="Siju K S">
<meta name="citation_author" content="Dr. Vipin V.">
<meta name="citation_publication_date" content="2024-09-17">
<meta name="citation_cover_date" content="2024-09-17">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-09-17">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="Center for Computational Engineering and Networking">
<meta name="citation_reference" content="citation_title=Literate programming;,citation_author=Donald E. Knuth;,citation_publication_date=1984-05;,citation_cover_date=1984-05;,citation_year=1984;,citation_fulltext_html_url=https://doi.org/10.1093/comjnl/27.2.97;,citation_issue=2;,citation_doi=10.1093/comjnl/27.2.97;,citation_issn=0010-4620;,citation_volume=27;,citation_journal_title=Comput. J.;,citation_publisher=Oxford University Press, Inc.;">
<meta name="citation_reference" content="citation_title=Automatic Tuning of Denoising Algorithms Parameters without Ground Truth;,citation_author=Arthur Floquet;,citation_author=Sayantan Dutta;,citation_author=Emmanuel Soubies;,citation_author=Duong-Hung Pham;,citation_author=Denis Kouamé;,citation_author=Denis Kouame;,citation_publication_date=2024-01;,citation_cover_date=2024-01;,citation_year=2024;,citation_fulltext_html_url=https://hal.science/hal-04344047;,citation_doi=10.1109/LSP.2024.3354554;,citation_volume=31;,citation_journal_title=IEEE Signal Processing Letters;,citation_publisher=Institute of Electrical and Electronics Engineers;">
<meta name="citation_reference" content="citation_title=Image denoising by sparse 3-d transform-domain collaborative filtering;,citation_author=Kostadin Dabov;,citation_author=Alessandro Foi;,citation_author=Vladimir Katkovnik;,citation_author=Karen Egiazarian;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_issue=8;,citation_doi=10.1109/TIP.2007.901238;,citation_volume=16;,citation_journal_title=IEEE Transactions on Image Processing;">
<meta name="citation_reference" content="citation_title=Total variation denoising via the moreau envelope;,citation_author=Ivan Selesnick;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=2;,citation_doi=10.1109/LSP.2017.2647948;,citation_volume=24;,citation_journal_title=IEEE Signal Processing Letters;">
<meta name="citation_reference" content="citation_title=Map-informed unrolled algorithms for hyper-parameter estimation;,citation_author=Pascal Nguyen;,citation_author=Emmanuel Soubies;,citation_author=Caroline Chaux;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_doi=10.1109/ICIP49359.2023.10222154;,citation_conference_title=2023 IEEE international conference on image processing (ICIP);">
<meta name="citation_reference" content="citation_title=Noisy-as-clean: Learning self-supervised denoising from corrupted image;,citation_author=Jun Xu;,citation_author=Yuan Huang;,citation_author=Ming-Ming Cheng;,citation_author=Li Liu;,citation_author=Fan Zhu;,citation_author=Zhou Xu;,citation_author=Ling Shao;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1109/TIP.2020.3026622;,citation_volume=29;,citation_journal_title=IEEE Transactions on Image Processing;">
<meta name="citation_reference" content="citation_title=Noisier2Noise: Learning to denoise from unpaired noisy data;,citation_author=Nick Moran;,citation_author=Dan Schmidt;,citation_author=Yu Zhong;,citation_author=Patrick Coady;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1109/CVPR42600.2020.01208;,citation_conference_title=2020 IEEE/CVF conference on computer vision and pattern recognition (CVPR);">
<meta name="citation_reference" content="citation_title=Recorrupted-to-recorrupted: Unsupervised deep learning for image denoising;,citation_author=Tongyao Pang;,citation_author=Huan Zheng;,citation_author=Yuhui Quan;,citation_author=Hui Ji;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.1109/CVPR46437.2021.00208;,citation_conference_title=2021 IEEE/CVF conference on computer vision and pattern recognition (CVPR);">
<meta name="citation_reference" content="citation_title=Monte-carlo sure: A black-box optimization of regularization parameters for general denoising algorithms;,citation_author=Sathish Ramani;,citation_author=Thierry Blu;,citation_author=Michael Unser;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=9;,citation_doi=10.1109/TIP.2008.2001404;,citation_volume=17;,citation_journal_title=IEEE Transactions on Image Processing;">
<meta name="citation_reference" content="citation_title=Automatic parameter selection for denoising algorithms using a no-reference measure of image content;,citation_author=Xiang Zhu;,citation_author=Peyman Milanfar;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_issue=12;,citation_doi=10.1109/TIP.2010.2052820;,citation_volume=19;,citation_journal_title=IEEE Transactions on Image Processing;">
<meta name="citation_reference" content="citation_title=Quantum mechanics-based signal and image representation: Application to denoising;,citation_author=Sayantan Dutta;,citation_author=Adrian Basarab;,citation_author=Bertrand Georgeot;,citation_author=Denis Kouamé;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.1109/OJSP.2021.3067507;,citation_volume=2;,citation_journal_title=IEEE Open Journal of Signal Processing;">
<meta name="citation_reference" content="citation_title=Image denoising inspired by quantum many-body physics;,citation_author=Sayantan Dutta;,citation_author=Adrian Basarab;,citation_author=Bertrand Georgeot;,citation_author=Denis Kouamé;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.1109/ICIP42928.2021.9506794;,citation_conference_title=2021 IEEE international conference on image processing (ICIP);">
<meta name="citation_reference" content="citation_title=Noise2Void - learning denoising from single noisy images;,citation_author=Alexander Krull;,citation_author=Tim-Oliver Buchholz;,citation_author=Florian Jug;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_doi=10.1109/CVPR.2019.00223;,citation_conference_title=2019 IEEE/CVF conference on computer vision and pattern recognition (CVPR);">
<meta name="citation_reference" content="citation_title=Noise2noise: Learning image restoration without clean data;,citation_author=J Lehtinen;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_journal_title=arXiv preprint arXiv:1803.04189;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Automatic Turning of Denoising Algorithms Parameters Without Ground Truth</h1>
            <p class="subtitle lead">An initial review for literature survey</p>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliations</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Siju K S <a href="mailto:ks_siju@cb.students.amrita.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0009-0004-1983-5574" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        School of Artifical Intelligence
                      </p>
                    <p class="affiliation">
                        Amrita Vishwa Vidyapeetham
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Dr.&nbsp;Vipin V. <a href="https://orcid.org/0000-0002-7859-8394" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        School of Artificial Intelligence
                      </p>
                    <p class="affiliation">
                        Amrita Vishwa Vidyapeetham
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">September 17, 2024</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></p></div><div class="quarto-title-meta-contents"><p><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF (agu)</a></p></div><div class="quarto-title-meta-contents"><p><a href="index-meca.zip" data-meca-link="true"><i class="bi bi-archive"></i>MECA Bundle</a></p></div></div></div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>This review explores the mathematical concepts, noise types, and denoising algorithms, focusing on additive Gaussian noise. The paper’s methodology was reproduced using Gaussian smoothing, with MSE and PSNR metrics. Optimization of the error function was performed using various techniques, including gradient descent and <code>scipy</code> minimization, highlighting the performance of simple denoisers.</p>
      </div>
    </div>

    <div>
      <div class="keywords">
        <div class="block-title">Keywords</div>
        <p>Bilevel optimization, Denoising, Hyper-parameter tuning, Additive noise, Gaussian smoothing, Denoising algorithms, Mean squared error (MSE), Peak signal-to-noise ratio (PSNR), Gradient descent, Finite difference method, Optimization</p>
      </div>
    </div>

    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#background-and-inspritaion-for-the-work" id="toc-background-and-inspritaion-for-the-work" class="nav-link" data-scroll-target="#background-and-inspritaion-for-the-work"><span class="header-section-number">2</span> Background and inspritaion for the work</a></li>
  <li><a href="#core-contents" id="toc-core-contents" class="nav-link" data-scroll-target="#core-contents"><span class="header-section-number">3</span> Core Contents</a>
  <ul class="collapse">
  <li><a href="#reference-models" id="toc-reference-models" class="nav-link" data-scroll-target="#reference-models"><span class="header-section-number">3.1</span> Reference models</a></li>
  <li><a href="#major-contribution" id="toc-major-contribution" class="nav-link" data-scroll-target="#major-contribution"><span class="header-section-number">3.2</span> Major contribution</a></li>
  <li><a href="#review-approach" id="toc-review-approach" class="nav-link" data-scroll-target="#review-approach"><span class="header-section-number">3.3</span> Review approach</a>
  <ul class="collapse">
  <li><a href="#general-form-of-loss-function" id="toc-general-form-of-loss-function" class="nav-link" data-scroll-target="#general-form-of-loss-function"><span class="header-section-number">3.3.1</span> General Form of loss function</a></li>
  <li><a href="#context-of-the-work" id="toc-context-of-the-work" class="nav-link" data-scroll-target="#context-of-the-work"><span class="header-section-number">3.3.2</span> Context of the work</a></li>
  </ul></li>
  <li><a href="#loss-functions-and-inference-schemes" id="toc-loss-functions-and-inference-schemes" class="nav-link" data-scroll-target="#loss-functions-and-inference-schemes"><span class="header-section-number">3.4</span> Loss functions and Inference schemes</a>
  <ul class="collapse">
  <li><a href="#optimization-of-loss-function" id="toc-optimization-of-loss-function" class="nav-link" data-scroll-target="#optimization-of-loss-function"><span class="header-section-number">3.4.1</span> Optimization of loss function</a></li>
  <li><a href="#presented-use-case" id="toc-presented-use-case" class="nav-link" data-scroll-target="#presented-use-case"><span class="header-section-number">3.4.2</span> Presented use case</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">4</span> Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">5</span> References</a></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="notebooks\review-preview.html"><i class="bi bi-journal-code"></i>Basics of Noise</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>This review undertakes a detailed exploration of the paper titled <em>Automatic Tuning of Denoising Algorithms Parameters without Ground Truth</em>. The primary objective of this work is not to propose a novel denoiser but rather to develop a framework for automatically tuning the hyperparameters of existing denoising algorithms using only the noisy input image, eliminating the need for clean reference images. This paradigm shift offers a unique unsupervised approach to parameter selection, which is of significant interest in real-world applications where ground truth images are often unavailable.</p>
</section>
<section id="background-and-inspritaion-for-the-work" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Background and inspritaion for the work</h1>
<p>This work is inspired by the supervised models like <em>Noise2Noise (N2N)</em> and <em>Noise as Clean (NaC)</em> , have been successful in estimating the denoised images through carefully defined loss functions optimized with access to clean or synthetic reference images. In contrast, the proposed method introduces novel <em>unsupervised loss functions</em> that allow the system to infer optimal hyperparameters directly from the noisy data.</p>
</section>
<section id="core-contents" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Core Contents</h1>
<p>This review attempts to recreate and dissect the theoretical framework and algorithmic implementations discussed in the paper. In particular, I focus on the key differences between the proposed method and the following models:</p>
<section id="reference-models" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="reference-models"><span class="header-section-number">3.1</span> Reference models</h2>
<ul>
<li><strong>Noise2Noise (N2N)</strong>: A supervised learning method where the target output is a noisy version of the input image, and the denoising model learns to map between these noisy inputs.</li>
<li><strong>Noise as Clean (NaC)</strong>: Where the noisy input is treated as if it were clean, allowing for simpler loss function optimizations but often at the loss of performance.</li>
<li><strong>Noiser to Noise (Nr2N)</strong>: A semi-supervised approach where multiple noisy versions of the same image are used to train the denoiser.</li>
<li><strong>R2R</strong>: A more robust method that uses random rotations to regularize and train the model for better generalization in real-world scenarios.</li>
</ul>
</section>
<section id="major-contribution" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="major-contribution"><span class="header-section-number">3.2</span> Major contribution</h2>
<p>The critical contribution of the reviewed paper lies in proposing alternative <em>unsupervised loss functions</em> and an <em>inference scheme</em> that automatically selects the hyperparameters such that the results empirically match those obtained through supervised methods. This approach demonstrates that comparable performance to supervised models can be achieved without access to clean reference data, leading to a potential breakthrough in how denoising algorithms are deployed in practical scenarios.</p>
</section>
<section id="review-approach" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="review-approach"><span class="header-section-number">3.3</span> Review approach</h2>
<p>This review compares the following aspects of the proposed methodology with existing denoising techniques:</p>
<ol type="1">
<li><p><em>loss Function Definition and Optimization</em>: Unlike the supervised models, where loss functions like Mean Squared Error (MSE) or structural similarity are optimized against clean images, the unsupervised loss functions in this paper rely on indirect metrics such as residual variance and image sharpness to estimate the quality of the denoised output.</p></li>
<li><p><em>Inference Scheme</em>: While supervised methods explicitly optimize their denoising algorithms based on the availability of paired clean and noisy data, the proposed inference scheme iteratively adjusts hyperparameters using gradient-based optimization on the unsupervised loss functions. The optimization process aims to converge on the denoised image <span class="math inline">\(\hat{x} \coloneqq x^*\)</span>, which matches the empirical quality of the ground truth.</p></li>
</ol>
<section id="general-form-of-loss-function" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="general-form-of-loss-function"><span class="header-section-number">3.3.1</span> General Form of loss function</h3>
<p>Denoising algorithms are of the form <span class="math inline">\(A_\theta (y)\)</span> , where <span class="math inline">\(\theta\in \mathbb{R}^d\)</span>. To estimate the parameter <span class="math inline">\(\theta\)</span> mostly generate mappings of the form <span class="math display">\[\Theta_\lambda:y\longrightarrow \theta\]</span> with parameters <span class="math inline">\(\lambda\in \mathbb{R}^d\)</span>. In short, this mapping maps image and its features to the set of parameters.</p>
<p>These mapping parameters are found by optimizing the average error produced by a discrepancy function, <span class="math inline">\(\mathcal{L}\)</span>. Using the modern computational terminology, this process is to find the optimal parameters, <span class="math inline">\(\lambda^*\)</span> such that <span class="math display">\[\lambda^*=\underset{\lambda\in \mathbb{R}^d}{\mathrm{argmin}}\,  \mathbb{E}\left(\mathcal{L}(A_{\theta_\lambda}(y)(y),x)\right)\]</span></p>
<p>Previous approaches demand a dataset for training. But this may not be possible in real situations. The new approach proposed in this article is unsupervised and is in line with the supervised models proposed in Noise to Noise (N2N), Noise as Clean (NaC), Noiser to Noise (Nr2N) and Recorrupted to Recorrupted (R2R).</p>
<p>Main thread of the work is that this novel approach defined an un-supervised loss (not depends on the ground truth <span class="math inline">\(x\)</span>), achieving the same minimizer <span class="math inline">\(\lambda^*\)</span> as the supervised counterpart.</p>
</section>
<section id="context-of-the-work" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="context-of-the-work"><span class="header-section-number">3.3.2</span> Context of the work</h3>
<p>The inspired works are supervised and have the disadvantages of overfitting and (or) non-generalizability with reference to a finite dataset <span class="math inline">\(\Omega_{i,j}\)</span>. Authors claim that, in the proposed unsupervised approach the parameters are time tuned and directly optimizing the loss function. The work is divided into two stages:</p>
<ol type="1">
<li>Define the loss function <span class="math inline">\(A_\theta\)</span> in various setups with low cardinality(<span class="math inline">\(\theta\)</span>)/ pixel values.</li>
<li>Solve the optimization problem (minimizing the loss function using gradient descent method). For the gradient calculations, the have used automatic differentiation.</li>
</ol>
</section>
</section>
<section id="loss-functions-and-inference-schemes" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="loss-functions-and-inference-schemes"><span class="header-section-number">3.4</span> Loss functions and Inference schemes</h2>
<p>With reference to the four published articles, the authors proposed the following loss functions and inference schemes. Here they consider two noisy images.</p>
<ol type="1">
<li><em>Noise to Noise</em>: <span class="citation" data-cites="lehtinen2018noise2noise">(<a href="#ref-lehtinen2018noise2noise" role="doc-biblioref">Lehtinen 2018</a>)</span></li>
</ol>
<p>The loss function is</p>
<p><span class="math display">\[ \hat{\theta}=\underset{\theta}{\mathrm{argmin}}\, ||A_\theta (y)-y'||^2_2\]</span></p>
<p>and the inference scheme is</p>
<p><span class="math display">\[x^{N2N}\coloneqq A_{\hat{\theta}}(y)\simeq x^*\]</span></p>
<p>were <span class="math inline">\(y\)</span> and <span class="math inline">\(y'\)</span> are two noisy data defined by <span class="math inline">\(y=x+n_1\)</span> and <span class="math inline">\(y'=x+n_2\)</span>.</p>
<ol start="2" type="1">
<li><em>Noisy as Clean</em>:<span class="citation" data-cites="9210208">(<a href="#ref-9210208" role="doc-biblioref">Xu et al. 2020</a>)</span> The loss function is</li>
</ol>
<p><span class="math display">\[ \hat{\theta}=\underset{\theta}{\mathrm{argmin}}\, ||A_\theta (z)-y'||^2_2\]</span></p>
<p>and the inference scheme is</p>
<p><span class="math display">\[x^{NaC}\coloneqq A_{\hat{\theta}}(y)\simeq x^*\]</span></p>
<p>where <span class="math inline">\(z\)</span> is a dobly noisy data defined by <span class="math inline">\(z=y+n_s\)</span>.</p>
<ol start="3" type="1">
<li><em>Noiser to Noise</em>: <span class="citation" data-cites="9156650">(<a href="#ref-9156650" role="doc-biblioref">Moran et al. 2020</a>)</span></li>
</ol>
<p>The loss function is</p>
<p><span class="math display">\[ \hat{\theta}=\underset{\theta}{\mathrm{argmin}}\,\mathbb{E} \left(||A_\theta (z)-y'||^2_2\right)\]</span></p>
<p>and the inference scheme is</p>
<p><span class="math display">\[x^{Nr2R}\coloneqq \frac{(1+\alpha^2)A_\theta(z)-z}{\alpha^2}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This approach has no restriction on noise except additive one. But the noise level may high. To mitigate this artificially high noise, lower the variance level of <span class="math inline">\(n_s\)</span> as <span class="math inline">\(n_s\sim \mathcal{N}(0,\alpha\sigma)\)</span>.</p>
</div>
</div>
<ol start="4" type="1">
<li><em>Recurrupted to Recurrupted</em>: <span class="citation" data-cites="9577798">(<a href="#ref-9577798" role="doc-biblioref">Pang et al. 2021</a>)</span></li>
</ol>
<p>In this reference, the noisy images are ‘doubly noisy’ images created from the clear image as <span class="math display">\[\begin{align*}
z_1&amp;=y+D^Tn_s\\
z_2&amp;=y-D^{-1}n_s
\end{align*}\]</span> with <span class="math inline">\(D\)</span> being any invertible matrix and <span class="math inline">\(n_s\)</span> drawn from same distribution of <span class="math inline">\(n\)</span>.</p>
<p>As a result, <span class="math inline">\(z_1=x+n_1\)</span> and <span class="math inline">\(z_2=x+n_2\)</span>, where <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are two zero mean independent noise vectors.</p>
<p>The loss function is</p>
<p><span class="math display">\[ \hat{\theta}=\underset{\theta}{\mathrm{argmin}}\,\mathbb{E} \left(||A_{\hat{\theta}} (z_1)-z_2||^2_2\right)\]</span></p>
<p>and the inference scheme is</p>
<p><span class="math display">\[x^{Nr2R}\coloneqq \frac{1}{M}\sum\limits_{m=1}^MA_{\hat{\theta}}(Z_1^m)\]</span></p>
<section id="optimization-of-loss-function" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="optimization-of-loss-function"><span class="header-section-number">3.4.1</span> Optimization of loss function</h3>
<p>For the optimization, the authors used gradient based approach. For the evaluation of gradient, they used automatic differentiation and the iterative formula for <span class="math inline">\(\theta\)</span> update is:</p>
<p><span class="math display">\[\theta^{n+1}=\theta_n-\eta \nabla \hat{\theta}\]</span></p>
<p>Here <span class="math inline">\(theta_0\)</span>, the initial parameter measure is found by manually tuning <span class="math inline">\(\theta\)</span> for a single image.</p>
</section>
<section id="presented-use-case" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="presented-use-case"><span class="header-section-number">3.4.2</span> Presented use case</h3>
<p>Authors used the proposed method on the denoiser, <em>Denoising via Quantum Interactive Patches</em> (DeQuIP) to fine tune the parameters. Implementation is done on <code>PyTorch 1.12.0</code> with BSD400 datasets as ground truth. Unfortunately, <code>Pytorch 1.12.0</code> is not connected to <code>Python 11.2</code> version. As per authors claim, the proposed approach makes it possible to obtain an average PSNR output within less than 1% of the best achievable PSNR. In this review work, a miniature model is developed using the authors concept.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-fig-denoising-plot" class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">
<div id="fig-denoising-plot" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A visualization of parameter fine tuning.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-denoising-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/notebooks-review-fig-denoising-plot-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Example of denoising results."><img src="index_files/figure-html/notebooks-review-fig-denoising-plot-output-1.png" alt="A visualization of parameter fine tuning." width="526" height="471" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-denoising-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Example of denoising results.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Skill of the proposed alogorithm on the same sample image used by the author is shown in <a href="#fig-denoising-plot" class="quarto-xref">Figure&nbsp;1</a></p>
</section>
</section>
</section>
<section id="conclusion" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Conclusion</h1>
<p>The review examined several influential works that inspired the authors’ unsupervised denoising framework, including Noise2Noise (N2N), Noise as Clean (NaC), and Noise2Noise Regression (Nr2N). These models demonstrated how effective denoising can be achieved without relying on clean ground-truth images, focusing solely on noisy data. By building on these ideas, the authors introduced novel unsupervised cost functions and inference schemes to match the performance of supervised denoising models. Using Gaussian smoothing as a basic case study, the review reproduced these methods and explored the optimization of the error functions through scipy minimization and custom gradient descent. Metrics such as MSE and PSNR provided a comparative analysis, reinforcing that while the unsupervised method closely mirrors the results of supervised models, further refinement is needed to fully realize its potential in more complex scenarios.</p>
</section>
<section id="references" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-4271520" class="csl-entry" role="listitem">
Dabov, Kostadin, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. 2007. <span>“Image Denoising by Sparse 3-d Transform-Domain Collaborative Filtering.”</span> <em>IEEE Transactions on Image Processing</em> 16 (8): 2080–95. <a href="https://doi.org/10.1109/TIP.2007.901238">https://doi.org/10.1109/TIP.2007.901238</a>.
</div>
<div id="ref-9506794" class="csl-entry" role="listitem">
Dutta, Sayantan, Adrian Basarab, Bertrand Georgeot, and Denis Kouamé. 2021a. <span>“Image Denoising Inspired by Quantum Many-Body Physics.”</span> In <em>2021 IEEE International Conference on Image Processing (ICIP)</em>, 1619–23. <a href="https://doi.org/10.1109/ICIP42928.2021.9506794">https://doi.org/10.1109/ICIP42928.2021.9506794</a>.
</div>
<div id="ref-9382109" class="csl-entry" role="listitem">
———. 2021b. <span>“Quantum Mechanics-Based Signal and Image Representation: Application to Denoising.”</span> <em>IEEE Open Journal of Signal Processing</em> 2: 190–206. <a href="https://doi.org/10.1109/OJSP.2021.3067507">https://doi.org/10.1109/OJSP.2021.3067507</a>.
</div>
<div id="ref-floquet:hal-04344047" class="csl-entry" role="listitem">
Floquet, Arthur, Sayantan Dutta, Emmanuel Soubies, Duong-Hung Pham, Denis Kouamé, and Denis Kouame. 2024. <span>“<span class="nocase">Automatic Tuning of Denoising Algorithms Parameters without Ground Truth</span>.”</span> <em><span>IEEE Signal Processing Letters</span></em> 31 (January): 381–85. <a href="https://doi.org/10.1109/LSP.2024.3354554">https://doi.org/10.1109/LSP.2024.3354554</a>.
</div>
<div id="ref-knuth84" class="csl-entry" role="listitem">
Knuth, Donald E. 1984. <span>“Literate Programming.”</span> <em>Comput. J.</em> 27 (2): 97–111. <a href="https://doi.org/10.1093/comjnl/27.2.97">https://doi.org/10.1093/comjnl/27.2.97</a>.
</div>
<div id="ref-8954066" class="csl-entry" role="listitem">
Krull, Alexander, Tim-Oliver Buchholz, and Florian Jug. 2019. <span>“Noise2Void - Learning Denoising from Single Noisy Images.”</span> In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2124–32. <a href="https://doi.org/10.1109/CVPR.2019.00223">https://doi.org/10.1109/CVPR.2019.00223</a>.
</div>
<div id="ref-lehtinen2018noise2noise" class="csl-entry" role="listitem">
Lehtinen, J. 2018. <span>“Noise2noise: Learning Image Restoration Without Clean Data.”</span> <em>arXiv Preprint arXiv:1803.04189</em>.
</div>
<div id="ref-9156650" class="csl-entry" role="listitem">
Moran, Nick, Dan Schmidt, Yu Zhong, and Patrick Coady. 2020. <span>“Noisier2Noise: Learning to Denoise from Unpaired Noisy Data.”</span> In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 12061–69. <a href="https://doi.org/10.1109/CVPR42600.2020.01208">https://doi.org/10.1109/CVPR42600.2020.01208</a>.
</div>
<div id="ref-10222154" class="csl-entry" role="listitem">
Nguyen, Pascal, Emmanuel Soubies, and Caroline Chaux. 2023. <span>“Map-Informed Unrolled Algorithms for Hyper-Parameter Estimation.”</span> In <em>2023 IEEE International Conference on Image Processing (ICIP)</em>, 2160–64. <a href="https://doi.org/10.1109/ICIP49359.2023.10222154">https://doi.org/10.1109/ICIP49359.2023.10222154</a>.
</div>
<div id="ref-9577798" class="csl-entry" role="listitem">
Pang, Tongyao, Huan Zheng, Yuhui Quan, and Hui Ji. 2021. <span>“Recorrupted-to-Recorrupted: Unsupervised Deep Learning for Image Denoising.”</span> In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2043–52. <a href="https://doi.org/10.1109/CVPR46437.2021.00208">https://doi.org/10.1109/CVPR46437.2021.00208</a>.
</div>
<div id="ref-4598837" class="csl-entry" role="listitem">
Ramani, Sathish, Thierry Blu, and Michael Unser. 2008. <span>“Monte-Carlo Sure: A Black-Box Optimization of Regularization Parameters for General Denoising Algorithms.”</span> <em>IEEE Transactions on Image Processing</em> 17 (9): 1540–54. <a href="https://doi.org/10.1109/TIP.2008.2001404">https://doi.org/10.1109/TIP.2008.2001404</a>.
</div>
<div id="ref-7807310" class="csl-entry" role="listitem">
Selesnick, Ivan. 2017. <span>“Total Variation Denoising via the Moreau Envelope.”</span> <em>IEEE Signal Processing Letters</em> 24 (2): 216–20. <a href="https://doi.org/10.1109/LSP.2017.2647948">https://doi.org/10.1109/LSP.2017.2647948</a>.
</div>
<div id="ref-9210208" class="csl-entry" role="listitem">
Xu, Jun, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, and Ling Shao. 2020. <span>“Noisy-as-Clean: Learning Self-Supervised Denoising from Corrupted Image.”</span> <em>IEEE Transactions on Image Processing</em> 29: 9316–29. <a href="https://doi.org/10.1109/TIP.2020.3026622">https://doi.org/10.1109/TIP.2020.3026622</a>.
</div>
<div id="ref-5484579" class="csl-entry" role="listitem">
Zhu, Xiang, and Peyman Milanfar. 2010. <span>“Automatic Parameter Selection for Denoising Algorithms Using a No-Reference Measure of Image Content.”</span> <em>IEEE Transactions on Image Processing</em> 19 (12): 3116–32. <a href="https://doi.org/10.1109/TIP.2010.2052820">https://doi.org/10.1109/TIP.2010.2052820</a>.
</div>
</div>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{k_s2024,
  author = {K S, Siju and Vipin V., Dr.},
  title = {Automatic {Turning} of {Denoising} {Algorithms} {Parameters}
    {Without} {Ground} {Truth}},
  date = {2024-09-17},
  langid = {en},
  abstract = {This review explores the mathematical concepts, noise
    types, and denoising algorithms, focusing on additive Gaussian
    noise. The paper’s methodology was reproduced using Gaussian
    smoothing, with MSE and PSNR metrics. Optimization of the error
    function was performed using various techniques, including gradient
    descent and `scipy` minimization, highlighting the performance of
    simple denoisers.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-k_s2024" class="csl-entry quarto-appendix-citeas" role="listitem">
K S, Siju, and Dr. Vipin V. 2024. <span>“Automatic Turning of Denoising
Algorithms Parameters Without Ground Truth.”</span> Center for
Computational Engineering and Networking. September 17, 2024.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","closeEffect":"zoom","openEffect":"zoom","loop":false,"descPosition":"bottom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>