{
  "hash": "1f2b79d206986ca0ff05e44fec4c9c9c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Basics of Noise\"\nauthor:\n  - name: Siju K S\n    affiliation: \n      - Research Scholar\n      - Center for Computational Engineering & Networking\n      - Amrita Vishwa Vidyapeetham\nnumber-sections: true\njupyter: python3\n---\n\n# Basics of Denoising Algorithms\n\nNoise is an unwanted random variation that interferes with the underlying signal in an image or data. In the context of image processing and denoising algorithms, noise introduces distortions that degrade the visual quality of an image, making it challenging to interpret or analyse the data accurately. Noise can arise from various sources during image acquisition, transmission, or processing. Understanding the types of noise and their mathematical representations is essential for designing effective denoising algorithms.\n\n## Types of Noise\n\n### 1. Additive Noise\n\nAdditive noise refers to noise that is simply added to the original signal. This type of noise is mathematically expressed as:\n\n$$\ny(i, j) = x(i, j) + n(i, j)\n$$\n\nWhere:\n- $y(i, j)$ is the noisy image,\n- $x(i, j)$ is the true, noise-free image, and\n- $n(i, j)$ is the noise at pixel \\((i, j)\\).\n\n#### Gaussian Noise\n\nGaussian noise is the most common type of additive noise. The noise at each pixel follows a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$:\n\n$$\nn(i, j) \\sim \\mathcal{N}(\\mu, \\sigma^2)\n$$\n\nIn most cases, Gaussian noise is assumed to have zero mean ($\\mu = 0$), making the noise purely random but distributed symmetrically around zero.\n\n### 2. Multiplicative Noise\n\nMultiplicative noise, as the name suggests, multiplies the signal rather than adding to it. This type of noise is often encountered in systems like radar and ultrasound imaging. It can be represented mathematically as:\n\n$$\ny(i, j) = x(i, j) \\cdot n(i, j)\n$$\n\n#### Speckle Noise\n\nA specific example of multiplicative noise is **speckle noise**, which typically arises in coherent imaging systems like laser and ultrasound. It is often modelled as:\n\n$$\nn(i, j) \\sim \\mathcal{U}(1, \\sigma)\n$$\n\nWhere $\\mathcal{U}(1, \\sigma)$ is a uniform distribution.\n\n### 3. Salt-and-Pepper Noise (Impulse Noise)\n\nSalt-and-pepper noise is a type of impulse noise where pixels are randomly corrupted with extreme values (either 0 or 255 in an 8-bit grayscale image). This noise is represented as:\n\n$$\ny(i, j) =\n\\begin{cases}\n0, & \\text{with probability } p_1 \\\\\n255, & \\text{with probability } p_2 \\\\\nx(i, j), & \\text{otherwise}\n\\end{cases}\n$$\n\nThe pixels take on either the minimum or maximum values, making this noise appear as white and black specks in an image.\n\n### 4. Poisson Noise (Shot Noise)\n\nPoisson noise arises due to the discrete nature of photon counting in imaging systems. It is signal-dependent, meaning the amount of noise depends on the intensity of the signal. This is expressed using the Poisson distribution:\n\n$$\ny(i, j) \\sim \\text{Poisson}(\\lambda x(i, j))\n$$\n\nHere, $\\lambda$ is a scaling factor, and the noise follows a Poisson distribution based on the true intensity value $x(i, j)$.\n\n### 5. Quantization Noise\n\nQuantization noise occurs during the digitization process, where continuous signal values are rounded off to discrete levels. This is typically modelled as additive noise with a uniform distribution:\n\n$$\nn(i, j) \\sim \\mathcal{U}(-\\Delta/2, \\Delta/2)\n$$\n\nWhere $\\Delta$ is the step size of the quantization process.\n\n## Connection to Denoising Algorithms\n\nDenoising algorithms are designed to estimate the clean signal $x(i,j)$ from the noisy observations $y(i,j)$. The type and characteristics of the noise play a crucial role in selecting appropriate denoising methods. Some commonly used denoising methods include:\n\n- **Gaussian Smoothing**: Ideal for Gaussian noise removal,\n- **Median Filtering**: Effective against salt-and-pepper noise,\n- **Non-Local Means (NLM)** and **BM3D**: Advanced methods for a wide range of noise types.\n\n## Introduction to Basic Denoising Algorithms\n\nDenoising algorithms are essential in image processing to remove various types of noise that distort image quality. Each type of noise (Gaussian, salt-and-pepper, multiplicative, Poisson, etc.) demands different approaches. This section develops a conceptual foundation for understanding the core denoising algorithms used to tackle these noises. We explore their mathematical models, functioning, and the rationale behind each method.\n\n## Denoising Algorithms for Additive Noise\n\nSeveral methods exist to remove or reduce additive noise in images. Below are the most common techniques:\n\n### 1. Gaussian Smoothing (Low-pass Filtering)\n\nGaussian smoothing is a linear filter that reduces noise by averaging the pixels in the neighbourhood, weighted by a Gaussian function. The denoised image can be computed as:\n\n$$ \\hat{x}(i, j) = \\sum_{k,l} G(k, l) y(i-k, j-l) $$\n\nwhere $$G(k, l)$$ is the Gaussian kernel.\n\n### 2. Median Filtering\n\nMedian filtering is a non-linear method where the value of each pixel is replaced by the median value of the neighbouring pixels. This is particularly effective for noise like salt-and-pepper noise, but it can also handle Gaussian noise:\n\n$$ \\hat{x}(i, j) = \\text{median} \\{ y(i+k, j+l) \\} $$\n\n### 3. Wiener Filtering\n\nWiener filtering is designed for images affected by additive Gaussian noise and uses local statistical information to perform denoising:\n\n$$ \\hat{x}(i, j) = \\frac{H^*(u,v) S_{xx}(u,v)}{|H(u,v)|^2 S_{xx}(u,v) + S_{nn}(u,v)} $$\n\nwhere:\n- $$H(u, v)$$ is the degradation function,\n- $$S_{xx}(u,v)$$ and $$S_{nn}(u,v)$$ are the power spectra of the original signal and noise, respectively.\n\nThe Wiener filter is derived under the assumption that the signal \n$x$ and noise $n$ are uncorrelated, and both are stationary stochastic processes. Given this, the Wiener filter is designed to minimize the MSE between the true signal and the estimate by accounting for the signal's and noise's power spectral densities (PSDs).\n\nThe Wiener filter minimizes the MSE by balancing the contribution of the noisy data and the signal's inherent structure. It effectively applies more filtering where the noise dominates (i.e., where \n$S_{nn}(\\omega)$ is large) and less filtering where the signal is stronger (i.e., where $S_{xx}(\\omega)$ is large).\n\n:::{.callout-note}\n### Intuition\nIn simple terms, the Wiener filter achieves the optimal balance between noise suppression and signal preservation by minimizing the MSE. It effectively chooses a trade-off between removing as much noise as possible while still preserving the signal's details. This is why the Wiener filter can be said to minimize the cost function, \n\n$$\\Phi_\\lambda(y)=\\mathbb{E}\\left(||y-x||^2\\right)$$\n\n By solving this minimization problem using calculus of variations, we arrive at the Wiener filter formula,\n\n $$H(\\omega)=\\frac{S_{xx}(\\omega)}{S_{xx}(\\omega)+S_{nn}(\\omega)}$$\n\n:::\n\n## Python Implementation for Additive Noise and Denoising{#sec-denoising}\n\nBelow is a `Python` code example using `OpenCV` and `Scipy` to apply denoising algorithms on the famous 'Lena' image.\n\n::: {#75842686 .cell execution_count=1}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nfrom PIL import Image\nimport urllib.request\nurllib.request.urlretrieve('http://lenna.org/len_top.jpg',\"input.jpg\")\nimg1 = Image.open(\"input.jpg\") #loading first image\narr1 = np.array(img1)\n```\n:::\n\n\n::: {#cfb443a4 .cell execution_count=2}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Load the Lena image\nlena = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Adding Gaussian noise with a lower sigma (e.g., sigma=10 for less noise)\nnoisy_lena = add_gaussian_noise(lena, sigma=10)\n\n# Display noisy image\nplt.figure(figsize=(8, 5))\nplt.imshow(noisy_lena, cmap='gray')\nplt.title('Noisy Lena Image (Gaussian Noise, sigma=10)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-3-output-1.png){width=649 height=396}\n:::\n:::\n\n\n:::{#0a1c3769 .cell .markdown}\n1. Gaussian Smoothing in Python\nGaussian smoothing can be applied using the following code:\n:::\n\n::: {#a2c194c1 .cell execution_count=3}\n``` {}\n#| code-overflow: wrap\n# Apply Gaussian smoothing\nsmoothed_lena = cv2.GaussianBlur(noisy_lena, (5, 5), sigmaX=1)\n\n# Display the denoised image\nplt.figure(figsize=(8, 5))\nplt.title(\"Denoised Image (Gaussian Smoothing)\")\nplt.imshow(smoothed_lena, cmap='gray')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-4-output-1.png){width=649 height=396}\n:::\n:::\n\n\n:::{#08795a42 .cell .markdown}\n2. Median Filtering in Python\n\nTo apply median filtering:\n:::\n\n::: {#4b861325 .cell execution_count=4}\n``` {}\n#| code-overflow: wrap\n# Apply Median filtering\nmedian_lena = cv2.medianBlur(noisy_lena, 5)\n\n# Display the denoised image\nplt.figure(figsize=(8, 5))\nplt.title(\"Denoised Image (Median Filtering)\")\nplt.imshow(median_lena, cmap='gray')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-5-output-1.png){width=649 height=396}\n:::\n:::\n\n\n:::{#32f44e94 .cell .markdown}\n3. Non-Local means denoising\n:::\n\n::: {#e2df25af .cell execution_count=5}\n``` {}\n#| code-overflow: wrap\n# Apply Non-Local Means denoising (cv2.fastNlMeansDenoising for grayscale)\nnlmeans_denoised_lena = cv2.fastNlMeansDenoising(noisy_lena, h=10, templateWindowSize=7, searchWindowSize=21)\n\n# Display the result\nplt.figure(figsize=(8, 5))\nplt.imshow(nlmeans_denoised_lena, cmap='gray')\nplt.title('Non-Local Means Denoising')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-6-output-1.png){width=649 height=396}\n:::\n:::\n\n\n:::{#3b89d22d .cell .markdown}\n4. Bilateral filtering\n:::\n\n::: {#703040f5 .cell execution_count=6}\n``` {}\n#| code-overflow: wrap\n# Apply Bilateral Filter\nbilateral_denoised = cv2.bilateralFilter(noisy_lena, d=9, sigmaColor=75, sigmaSpace=75)\n\n# Display the result\nplt.figure(figsize=(8, 5))\nplt.imshow(bilateral_denoised, cmap='gray')\nplt.title('Bilateral Filtering')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-7-output-1.png){width=649 height=396}\n:::\n:::\n\n\n:::{#b0690716 .cell .markdown}\n3. Weinner denoiser\n:::\n\n::: {#91bf47c9 .cell execution_count=7}\n``` {}\n#| code-overflow: wrap\nfrom scipy.signal import wiener\n\n# Apply Wiener Filtering\ndenoised_image = wiener(noisy_lena)\n# Display the result\nplt.figure(figsize=(8, 5))\nplt.imshow(denoised_image, cmap='gray')\nplt.title('Weinner denoiser')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-8-output-1.png){width=758 height=458}\n:::\n:::\n\n\n:::{#4c287ac5 .cell .markdown}\n:::{.callout-note}\n### Reason for worst denoising effect\nIn this case, adding Gaussian noise makes the noise stationary, but if the underlying image is non-stationary (as most images are), the Wiener filter may struggle to achieve optimal results. It treats all areas of the image uniformly, without taking into account the local variations (edges, textures), which leads to suboptimal denoising.\n:::\n\nTo improve the performance of denoising in such cases, advanced techniques (e.g., BM3D, non-local means) that adapt to local variations in the image are generally more effective.\n\nFrom these illustrations, let's go for the simplest case but reasonably fit to our learning needs- the Gaussian smoothing. As an experiment to optimize the image denoising with optimal paramters, an iterative approach is used as shown below. Here the adopted procedure is:\n\n*Define a Metric for Error:* Calculate the error between the denoised image and the original image. Common metrics include Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR).\n\n*Vary Parameters:* Experiment with different values for the kernel size and sigmaX to see which combination provides the best performance.\n\n*Evaluate and Choose the Best Parameters:* Use the defined error metric to compare different parameter settings and select the ones with the lowest error.\n:::\n\n::: {#bb5f3216 .cell execution_count=8}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Load the Lena image\nlena = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to compute MSE\ndef compute_mse(original, denoised):\n    return mean_squared_error(original.flatten(), denoised.flatten())\n\n# Function to apply Gaussian smoothing\ndef apply_gaussian_smoothing(noisy_image, ksize, sigmaX):\n    return cv2.GaussianBlur(noisy_image, (ksize, ksize), sigmaX=sigmaX)\n\n# Create noisy image\nnoisy_lena = add_gaussian_noise(lena, sigma=10)\n\n# Define parameter ranges\nkernel_sizes = [3, 5, 7, 9]  # Example kernel sizes\nsigmaXs = [0.5, 1, 1.5, 2]   # Example sigmaX values\n\n# Variables to keep track of best parameters\nbest_mse = float('inf')\nbest_params = (None, None)\nbest_denoised_image = None\n\n# Prepare subplots\nfig, axes = plt.subplots(len(kernel_sizes), len(sigmaXs), figsize=(8, 5))\nfig.suptitle('Denoised Images for Different Parameters', fontsize=16)\n\nfor i, ksize in enumerate(kernel_sizes):\n    for j, sigmaX in enumerate(sigmaXs):\n        # Apply Gaussian smoothing\n        smoothed_lena = apply_gaussian_smoothing(noisy_lena, ksize, sigmaX)\n        \n        # Compute MSE\n        mse = compute_mse(lena, smoothed_lena)\n        \n        # Update best parameters if this is the best MSE\n        if mse < best_mse:\n            best_mse = mse\n            best_params = (ksize, sigmaX)\n            best_denoised_image = smoothed_lena\n        \n        # Display the image in subplot\n        ax = axes[i, j]\n        ax.imshow(smoothed_lena, cmap='gray')\n        ax.set_title(f'K={ksize}, S={sigmaX}')\n        ax.axis('off')\n\n# Display the best denoised image\nplt.figure()\nplt.title(\"Best Denoised Image (Gaussian Smoothing)\")\nplt.imshow(best_denoised_image, cmap='gray')\nplt.tight_layout()\nplt.show()\nprint(f\"Best parameters: Kernel Size = {best_params[0]}, SigmaX = {best_params[1]}\")\nprint(f\"Best MSE: {best_mse}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-9-output-1.png){width=614 height=433}\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-9-output-2.png){width=662 height=404}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters: Kernel Size = 5, SigmaX = 1\nBest MSE: 30.841855555555554\n```\n:::\n:::\n\n\n:::{#4585d4dd .cell .markdown}\nMean Square Error is a common but imperfect measure of image quality. MSE focuses on pixel-wise differences, which may not correlate well with perceptual image quality. An image can have low MSE but still appear blurry or lack important details. Newer algorithms often aim to balance MSE minimization with other perceptually relevant metrics, leading to better subjective image quality even if the MSE is slightly higher.\n\nPeak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. It is a common metric used to assess the quality of the denoised image defined as:\n\n$$ \\text{PSNR}=10\\log_{10}\\left(\\frac{\\text{Max}_i^2}{\\text{MSE}}\\right)$$\n\nwhere \n$\\text{MAX}_i$  is the maximum possible pixel value of the image, and MSE is the mean squared error between the ground truth and the denoised image. A `Python` implementation that replaces MSE with PSNR is shown below.\n:::\n\n::: {#71b35753 .cell execution_count=9}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Load the Lena image\nlena = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to compute PSNR\ndef compute_psnr(original, denoised):\n    mse = mean_squared_error(original.flatten(), denoised.flatten())\n    if mse == 0:\n        return 100  # Perfect match\n    max_pixel = 255.0\n    return 20 * np.log10(max_pixel / np.sqrt(mse))\n\n# Function to apply Gaussian smoothing\ndef apply_gaussian_smoothing(noisy_image, ksize, sigmaX):\n    return cv2.GaussianBlur(noisy_image, (ksize, ksize), sigmaX=sigmaX)\n\n# Create noisy image\nnoisy_lena = add_gaussian_noise(lena, sigma=10)\n\n# Define parameter ranges\nkernel_sizes = [3, 5, 7, 9]  # Example kernel sizes\nsigmaXs = [0.5, 1, 1.5, 2]   # Example sigmaX values\n\n# Variables to keep track of best parameters\nbest_psnr = -float('inf')\nbest_params = (None, None)\nbest_denoised_image = None\n\n# Prepare subplots\nfig, axes = plt.subplots(len(kernel_sizes), len(sigmaXs), figsize=(8, 5))\nfig.suptitle('Denoised Images for Different Parameters', fontsize=16)\n\nfor i, ksize in enumerate(kernel_sizes):\n    for j, sigmaX in enumerate(sigmaXs):\n        # Apply Gaussian smoothing\n        smoothed_lena = apply_gaussian_smoothing(noisy_lena, ksize, sigmaX)\n        \n        # Compute PSNR\n        psnr = compute_psnr(lena, smoothed_lena)\n        \n        # Update best parameters if this is the best PSNR\n        if psnr > best_psnr:\n            best_psnr = psnr\n            best_params = (ksize, sigmaX)\n            best_denoised_image = smoothed_lena\n        \n        # Display the image in subplot\n        ax = axes[i, j]\n        ax.imshow(smoothed_lena, cmap='gray')\n        ax.set_title(f'K={ksize}, S={sigmaX}')\n        ax.axis('off')\n\n# Display the best denoised image\nplt.figure()\nplt.title(\"Best Denoised Image (Gaussian Smoothing)\")\nplt.imshow(best_denoised_image, cmap='gray')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Best parameters: Kernel Size = {best_params[0]}, SigmaX = {best_params[1]}\")\nprint(f\"Best PSNR: {best_psnr} dB\")\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-10-output-1.png){width=614 height=433}\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-10-output-2.png){width=662 height=404}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters: Kernel Size = 3, SigmaX = 1\nBest PSNR: 33.24004484491318 dB\n```\n:::\n:::\n\n\n:::{#4398573e .cell .markdown}\nIn this experiment, both the approaches produced the same result!\n\nNext we consider a more general situation. Suppose 20 noisy images of the same clean image are avaiable. Need to denoise all the images at the best level. There are two approaches for this situation.\n\n1. Apply denoising algorithm with varying parameters on all 20 noisy images, and identify the best parameters using MSE or PSNR metric. Finally use these best parameters on all noised images to denoise to the optimum.\n\n2. Use a same approach in the first part. But instead of using the best parameters, use the parameters of the *most likely* MSE or PSNR measure for final denoising. \n\nPython implementation of the second approach is given below.\n:::\n\n::: {#687f3dfd .cell execution_count=10}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Load the Lena image\nlena = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to compute PSNR\ndef compute_psnr(original, denoised):\n    mse = mean_squared_error(original.flatten(), denoised.flatten())\n    if mse == 0:\n        return 100  # Perfect match\n    max_pixel = 255.0\n    return 20 * np.log10(max_pixel / np.sqrt(mse))\n\n# Function to apply Gaussian smoothing\ndef apply_gaussian_smoothing(noisy_image, ksize, sigmaX):\n    return cv2.GaussianBlur(noisy_image, (ksize, ksize), sigmaX=sigmaX)\n\n# Create 20 noisy images\nnum_noisy_images = 20\nnoisy_images = [add_gaussian_noise(lena, sigma=10) for _ in range(num_noisy_images)]\n\n# Define parameter ranges\nkernel_sizes = [3, 5, 7, 9]  # Example kernel sizes\nsigmaXs = [0.5, 1, 1.5, 2]   # Example sigmaX values\n\n# Variables to keep track of best parameters\nbest_psnr = -float('inf')\nbest_params = (None, None)\n\n# Evaluate parameters on the dataset\nfor ksize in kernel_sizes:\n    for sigmaX in sigmaXs:\n        psnr_total = 0\n        for noisy_img in noisy_images:\n            # Apply Gaussian smoothing\n            smoothed_img = apply_gaussian_smoothing(noisy_img, ksize, sigmaX)\n            \n            # Compute PSNR\n            psnr = compute_psnr(lena, smoothed_img)\n            psnr_total += psnr\n        \n        # Average PSNR for this parameter set\n        avg_psnr = psnr_total / num_noisy_images\n        \n        # Update best parameters if this is the best PSNR\n        if avg_psnr > best_psnr:\n            best_psnr = avg_psnr\n            best_params = (ksize, sigmaX)\n\nprint(f\"Best parameters: Kernel Size = {best_params[0]}, SigmaX = {best_params[1]}\")\nprint(f\"Best Average PSNR: {best_psnr} dB\")\n\n# Prepare subplots\nrows = 4\ncols = 5\nfig, axes = plt.subplots(rows, cols, figsize=(8, 5))\n\n# Flatten axes array for easy indexing\naxes = axes.flatten()\n\n# Apply optimal parameters to noisy images and display\nfor i, noisy_img in enumerate(noisy_images):\n    # Apply Gaussian smoothing with optimal parameters\n    denoised_img = apply_gaussian_smoothing(noisy_img, best_params[0], best_params[1])\n    \n    # Display the denoised image in subplot\n    ax = axes[i]\n    ax.imshow(denoised_img, cmap='gray')\n    ax.set_title(f\"Image {i+1}\")\n    ax.axis('off')\n\n# Hide any unused subplots\nfor j in range(num_noisy_images, rows * cols):\n    axes[j].axis('off')\n\n# Adjust layout and display the plot\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters: Kernel Size = 5, SigmaX = 1\nBest Average PSNR: 33.21379870244354 dB\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-11-output-2.png){width=758 height=442}\n:::\n:::\n\n\n:::{#7e741e70 .cell .markdown}\n## Landing to the background of the article\n\nIs it possible to create a high-quality denoised image from a set of noisy images, even if we don't have the clean reference image? \n\nSure. This classical technique is commonly referred to as image denoising through *ensemble methods* or *image fusion*. The idea is to leverage the fact that each noisy image contains some useful information, and by combining these images effectively, one can obtain a better denoised result. This seems to be more realistic in application view point.\n:::\n\n::: {#6b6d0176 .cell execution_count=11}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to compute Mean Squared Error (MSE) between two images\ndef compute_mse(image1, image2):\n    return mean_squared_error(image1.flatten(), image2.flatten())\n\n# Load the clean image\nlena = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Number of noisy images to generate\nnum_noisy_images = 20\nsigma = 10  # Standard deviation for Gaussian noise\n\n# Generate noisy images\nnoisy_images = [add_gaussian_noise(lena, sigma) for _ in range(num_noisy_images)]\n\n# Define parameter ranges for denoising\nkernel_sizes = [3, 5, 7, 9]\nsigmaXs = [0.5, 1, 1.5, 2]\n\n# Variables to keep track of best parameters\nbest_mse = float('inf')\nbest_params = (None, None)\nbest_denoised_images = []\n\n# Perform denoising for different parameters\nfor ksize in kernel_sizes:\n    for sigmaX in sigmaXs:\n        denoised_images = []\n        for noisy_img in noisy_images:\n            # Apply Gaussian smoothing\n            smoothed_img = cv2.GaussianBlur(noisy_img, (ksize, ksize), sigmaX=sigmaX)\n            denoised_images.append(smoothed_img)\n        \n        # Convert list to numpy array for easy manipulation\n        denoised_images = np.array(denoised_images)\n\n        # Compute the average of all denoised images\n        average_denoised_image = np.mean(denoised_images, axis=0)\n        average_denoised_image = np.clip(average_denoised_image, 0, 255).astype(np.uint8)\n\n        # Compute the mean squared error (MSE) between all pairs of denoised images\n        mse_total = 0\n        count = 0\n        for i in range(len(denoised_images)):\n            for j in range(i + 1, len(denoised_images)):\n                mse_total += compute_mse(denoised_images[i], denoised_images[j])\n                count += 1\n        \n        # Average MSE\n        mse_avg = mse_total / count if count > 0 else float('inf')\n\n        # Update best parameters if this is the best MSE\n        if mse_avg < best_mse:\n            best_mse = mse_avg\n            best_params = (ksize, sigmaX)\n            best_denoised_images = denoised_images\n\n# Compute the average image from the best denoised images\naverage_best_denoised_image = np.mean(best_denoised_images, axis=0)\naverage_best_denoised_image = np.clip(average_best_denoised_image, 0, 255).astype(np.uint8)\n\n# Display the averaged denoised images\nfig, axes = plt.subplots(4, 5, figsize=(8, 5))\nfor i in range(4):\n    for j in range(5):\n        idx = i * 5 + j\n        if idx < len(noisy_images):\n            axes[i, j].imshow(noisy_images[idx], cmap='gray')\n            axes[i, j].set_title(f'Noisy Image {idx+1}')\n            axes[i, j].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Display the best denoised image\nplt.figure(figsize=(8, 5))\nplt.imshow(average_best_denoised_image, cmap='gray')\nplt.title(f\"Best Averaged Denoised Image\\nBest Parameters: K={best_params[0]}, S={best_params[1]}\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Print best parameters and MSE\nprint(f\"Best parameters: Kernel Size = {best_params[0]}, SigmaX = {best_params[1]}\")\nprint(f\"Best Average MSE: {best_mse}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-12-output-1.png){width=758 height=457}\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-12-output-2.png){width=758 height=473}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters: Kernel Size = 9, SigmaX = 2\nBest Average MSE: 4.6294501754386\n```\n:::\n:::\n\n\n:::{#b65426a7 .cell .markdown}\nInstead of MSE, the PSNR metric can be used. Here the comparison is with another *noisy image*.\n:::\n\n::: {#d7436efe .cell execution_count=12}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Function to calculate PSNR\ndef calculate_psnr(original, denoised):\n    mse = np.mean((original - denoised) ** 2)\n    if mse == 0:\n        return float('inf')  # PSNR is infinite for identical images\n    max_pixel = 255.0\n    psnr = 10 * np.log10((max_pixel ** 2) / mse)\n    return psnr\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Load the clean image\nlena = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Number of noisy images to generate\nnum_noisy_images = 20\nsigma = 10  # Standard deviation for Gaussian noise\n\n# Generate noisy images\nnoisy_images = [add_gaussian_noise(lena, sigma) for _ in range(num_noisy_images)]\n\n# Define parameter ranges for denoising\nkernel_sizes = [3, 5, 7, 9]\nsigmaXs = [0.5, 1, 1.5, 2]\n\n# Variables to keep track of best parameters\nbest_psnr = float('-inf')\nbest_params = (None, None)\nbest_denoised_images = []\n\n# Perform denoising for different parameters\nfor ksize in kernel_sizes:\n    for sigmaX in sigmaXs:\n        denoised_images = []\n        for noisy_img in noisy_images:\n            # Apply Gaussian smoothing\n            smoothed_img = cv2.GaussianBlur(noisy_img, (ksize, ksize), sigmaX=sigmaX)\n            denoised_images.append(smoothed_img)\n        \n        # Convert list to numpy array for easy manipulation\n        denoised_images = np.array(denoised_images)\n\n        # Compute the average of all denoised images\n        average_denoised_image = np.mean(denoised_images, axis=0)\n        average_denoised_image = np.clip(average_denoised_image, 0, 255).astype(np.uint8)\n\n        # Compute the average PSNR between all pairs of denoised images\n        psnr_total = 0\n        count = 0\n        for i in range(len(denoised_images)):\n            for j in range(i + 1, len(denoised_images)):\n                psnr_total += calculate_psnr(denoised_images[i], denoised_images[j])\n                count += 1\n        \n        # Average PSNR\n        psnr_avg = psnr_total / count if count > 0 else float('-inf')\n\n        # Update best parameters if this is the best PSNR\n        if psnr_avg > best_psnr:\n            best_psnr = psnr_avg\n            best_params = (ksize, sigmaX)\n            best_denoised_images = denoised_images\n\n# Compute the average image from the best denoised images\naverage_best_denoised_image = np.mean(best_denoised_images, axis=0)\naverage_best_denoised_image = np.clip(average_best_denoised_image, 0, 255).astype(np.uint8)\n\n# Display the averaged denoised images\nfig, axes = plt.subplots(4, 5, figsize=(8, 5))\nfor i in range(4):\n    for j in range(5):\n        idx = i * 5 + j\n        if idx < len(noisy_images):\n            axes[i, j].imshow(noisy_images[idx], cmap='gray')\n            axes[i, j].set_title(f'Noisy Image {idx+1}')\n            axes[i, j].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Display the best denoised image\nplt.figure(figsize=(8, 5))\nplt.imshow(average_best_denoised_image, cmap='gray')\nplt.title(f\"Best Averaged Denoised Image\\nBest Parameters: K={best_params[0]}, S={best_params[1]}\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Print best parameters and PSNR\nprint(f\"Best parameters: Kernel Size = {best_params[0]}, SigmaX = {best_params[1]}\")\nprint(f\"Best Average PSNR: {best_psnr}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-13-output-1.png){width=758 height=457}\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-13-output-2.png){width=758 height=473}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters: Kernel Size = 9, SigmaX = 2\nBest Average PSNR: 41.510077911111544\n```\n:::\n:::\n\n\n:::{#0e939872 .cell .markdown}\nIn this article, authors refer that, in the context of image denoising with Gaussian noise, *Generalized Cross Validation (GCV)* and **Stein's Unbiased Risk Estimate (SURE)* are used to estimate the performance of denoising algorithms without requiring a clean reference image. A review of these topics from fundamentals of image processing provides following key insights.\n\n### Generalized Cross Validation (GCV)\n\nGeneralized Cross Validation is a technique used to estimate the optimal parameters of a model by minimizing an estimate of the prediction error. In image denoising, GCV helps in selecting the best parameters for a denoising algorithm without needing a clean reference image.\n\nSteps in using GCV are:\n\n1. *Model and Data*: Apply a denoising algorithm with different parameter settings to the noisy image.\n2. *Estimate Residuals*: For each parameter setting, compute an estimate of the residual error. This involves calculating how well the model predicts the noisy image based on the parameters.\n3. *Optimize Parameters*: Choose the parameters that minimize the generalized cross-validation criterion, which is an estimate of the prediction error.\n\n**Mathematical Formulation:**\n\nFor Gaussian noise, GCV often involves minimizing the following criterion:\n\n$$\n\\text{GCV}(\\lambda) = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i - \\hat{y}_i)^2}{(1 - h_i)^2}\n$$\n\nwhere:\n- $y_i$  is the observed noisy value.\n- $\\hat{y}_i$  is the estimated value from the denoising model.\n- $h_i$  is the leverage or influence of the $i$-th observation.\n\n### Stein's Unbiased Risk Estimate (SURE)\n\nStein's Unbiased Risk Estimate provides an estimate of the mean squared error (MSE) of an estimator without needing the true clean image. It is particularly useful for choosing regularization parameters in denoising problems.\n\nKey steps:\n\n1. *Estimate the Risk*: Compute Stein's unbiased risk estimate based on the noisy image and the denoised estimate.\n2. *Optimize Parameters*: Select the parameters that minimize the SURE estimate.\n\n**Mathematical Formulation:**\n\nSURE for Gaussian noise can be expressed as:\n\n$$\n\\text{SURE}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2 + 2 \\sigma^2 \\text{trace}(I - H)\n$$\n\nwhere:\n- $y_i$  is the noisy observation.\n- $\\hat{f}(x_i)$  is the estimate from the denoising algorithm.\n- $\\sigma^2$  is the noise variance.\n- $H$  is the hat matrix or influence matrix.\n\n:::{.callout-note}\n\n- *Generalized Cross Validation (GCV)* and *Stein's Unbiased Risk Estimate (SURE)* are statistical methods used for parameter selection in denoising algorithms without needing a clean reference image.\n\n- *GCV* involves minimizing an estimate of the prediction error.\n\n- *SURE* provides an unbiased estimate of the mean squared error for a given denoised image.\n\nBoth methods help in optimizing denoising parameters effectively in the absence of a clean image by leveraging statistical properties of Gaussian noise.\n:::\n\nAs in the previous examples, it is clear that the MSE is not a good measure for assessing quality of a denoised image and PSNR is more on perception side. So, experimenting the GCV and SURE is a good option.\n\nPython implementation of *GCV* is given below.\n:::\n\n::: {#11061b7c .cell execution_count=13}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Load the clean image\nclean_image = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to apply Gaussian smoothing\ndef apply_gaussian_smoothing(noisy_image, ksize, sigmaX):\n    return cv2.GaussianBlur(noisy_image, (ksize, ksize), sigmaX=sigmaX)\n\n# Function to compute Generalized Cross Validation (GCV)\ndef generalized_cross_validation(noisy_images, denoised_images, parameters):\n    best_gcv = float('inf')\n    best_denoised_image = None\n    best_params = None\n\n    for i, denoised_image in enumerate(denoised_images):\n        residuals = [noisy - denoised_image for noisy in noisy_images]\n        \n        # Calculate a simplified leverage matrix\n        h = np.ones_like(noisy_images[0]) * 0.01  # Small constant to prevent division by zero\n        \n        # Compute GCV criterion for each noisy image\n        gcv_values = []\n        for residual in residuals:\n            gcv = np.mean((residual ** 2) / (1 - h) ** 2)\n            gcv_values.append(gcv)\n        \n        average_gcv = np.mean(gcv_values)\n        \n        if average_gcv < best_gcv:\n            best_gcv = average_gcv\n            best_denoised_image = denoised_image\n            best_params = parameters[i]\n    \n    return best_denoised_image, best_params\n\n# Parameters\nsigma_values = [10]  # Gaussian noise standard deviation\nkernel_sizes = [3, 5, 7, 9]  # Example kernel sizes\nsigmaXs = [0.5, 1, 1.5, 2]   # Example sigmaX values\n\n# Generate multiple noisy images\nnum_noisy_images = 10  # Ensure a sufficient number of noisy images\nnoisy_images = [add_gaussian_noise(clean_image, sigma=sigma_values[0]) for _ in range(num_noisy_images)]\n\n# Apply Gaussian smoothing with different parameters\ndenoised_images = []\nparameters = []\nfor noisy_image in noisy_images:\n    for ksize in kernel_sizes:\n        for sigmaX in sigmaXs:\n            denoised_image = apply_gaussian_smoothing(noisy_image, ksize, sigmaX)\n            denoised_images.append(denoised_image)\n            parameters.append((ksize, sigmaX))\n\n# Check if there are enough noisy images to split\nif len(noisy_images) > 1:\n    # Split the noisy images into training and validation sets\n    train_noisy, val_noisy = train_test_split(noisy_images, test_size=0.5, random_state=42)\n\n    # Apply GCV to find the best denoised image\n    best_denoised_image, best_params = generalized_cross_validation(train_noisy, denoised_images, parameters)\nelse:\n    print(\"Not enough noisy images to perform training and validation split.\")\n    best_denoised_image = denoised_images[0]  # Default to the first denoised image if not enough data\n    best_params = parameters[0]  # Default to the first parameters if not enough data\n\n# Display the best denoised image\nplt.figure(figsize=(8, 5))\nplt.imshow(best_denoised_image, cmap='gray')\nplt.title(f\"Best Denoised Image (GCV)\\nK={best_params[0]}, S={best_params[1]}\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-14-output-1.png){width=758 height=473}\n:::\n:::\n\n\n:::{#d981405d .cell .markdown}\nPython implementation of SURE approach is shown below.\n:::\n\n::: {#457a6d20 .cell execution_count=14}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Load the clean image\nclean_image = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to apply Gaussian smoothing\ndef apply_gaussian_smoothing(noisy_image, ksize, sigmaX):\n    return cv2.GaussianBlur(noisy_image, (ksize, ksize), sigmaX=sigmaX)\n\n# Function to compute Stein’s Unbiased Risk Estimate (SURE)\ndef steins_unbiased_risk_estimate(noisy_image, denoised_image, sigma):\n    # Compute residuals\n    residuals = noisy_image - denoised_image\n    \n    # Estimate the risk\n    n = noisy_image.size\n    sure = np.mean(residuals ** 2) + 2 * sigma ** 2 * np.mean(np.abs(residuals))\n    \n    return sure\n\n# Parameters\nsigma_values = [10]  # Gaussian noise standard deviation\nkernel_sizes = [3, 5, 7, 9]  # Example kernel sizes\nsigmaXs = [0.5, 1, 1.5, 2]   # Example sigmaX values\n\n# Generate noisy images\nnoisy_images = [add_gaussian_noise(clean_image, sigma) for sigma in sigma_values]\n\n# Apply Gaussian smoothing with different parameters\ndenoised_images = []\nfor noisy_image in noisy_images:\n    for ksize in kernel_sizes:\n        for sigmaX in sigmaXs:\n            denoised_image = apply_gaussian_smoothing(noisy_image, ksize, sigmaX)\n            denoised_images.append((denoised_image, ksize, sigmaX))\n\n# Find the best denoised image using SURE\nbest_sure = float('inf')\nbest_denoised_image = None\nbest_params = (None, None)\n\nfor denoised_image, ksize, sigmaX in denoised_images:\n    sure = steins_unbiased_risk_estimate(noisy_image, denoised_image, sigma=10)\n    \n    if sure < best_sure:\n        best_sure = sure\n        best_denoised_image = denoised_image\n        best_params = (ksize, sigmaX)\n\n# Display the best denoised image\nplt.figure(figsize=(8, 5))\nplt.imshow(best_denoised_image, cmap='gray')\nplt.title(f\"Best Denoised Image (SURE)\\nK={best_params[0]}, S={best_params[1]}\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-15-output-1.png){width=758 height=473}\n:::\n:::\n\n\n:::{#c8d0e32c .cell .markdown}\nIn this experiment, both the approach come with same parameters!\n\nCross-validation is used to assess how well a model (or in this case, a denoising algorithm with specific parameters $k$ and `sigmaX`) generalizes to unseen data. For image denoising, it involves splitting the data into training and validation sets to evaluate the performance of different parameter settings.\n\nIn the context of denoising, we would use noisy images with known parameters to select the best parameters by evaluating how well the denoising algorithm performs on validation images. This process helps in selecting parameters that *generalize* well across different noisy samples.\n\nAlso *SURE* provides an estimate of the risk (or error) of a denoising algorithm based on the residuals between noisy and denoised images. It helps in selecting parameters that minimize this estimated risk.\n\nNow come into the authors' concept-*… the proposed method i.e., fitting algorithm parameters to a single image, corresponds to the extreme case of one pair of noisy images of a single clean image*. So, training on variable images is not possible. Here the parameter fine tuning in restricted to just a pair of noisy images of a single clean image which in turn not available for reference!\n\n## Replicating the author's work with gradient descend\n\nIn the proposed model for `N2N` denoiser, authors formulate the objective function as:\n$$\\mathcal{C}(\\theta)=||A_\\theta(y)-y'||$$\n\nWhere $A_\\theta$ is the denoiser algorithm in N2N model , $y$ and $y'$ are two noisy images. They minimize this objective function using gradient descent with automatic differentiation and initial value of the parameters $\\theta$, $\\theta_0$ found by manually tuning the parameters for a single image.\n\nTo optimize the cost function without using neural networks, we can directly minimize the loss function using optimization techniques (`minimize` function from `scipy` library). A python implementation of this parameter tuning is given below.\n:::\n\n::: {#ea4e4f04 .cell execution_count=15}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\n# Load the clean image\nclean_image = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Create noisy images\nsigma = 10\nnoisy_image1 = add_gaussian_noise(clean_image, sigma)\nnoisy_image2 = add_gaussian_noise(clean_image, sigma)\n## Define the Denoising Function- The quadratic error function\ndef apply_gaussian_filter(image, kernel_size, sigma):\n    return cv2.GaussianBlur(image, (kernel_size, kernel_size), sigmaX=sigma)\n\ndef cost_function(params, noisy_image1, noisy_image2):\n    kernel_size, sigma = int(params[0]), params[1]\n    denoised_image = apply_gaussian_filter(noisy_image1, kernel_size, sigma)\n    residuals = denoised_image - noisy_image2\n    return np.sum(residuals ** 2)\n## Optimize the Parameters using gradient descend\n# Initial guess for parameters (kernel_size, sigma)\ninitial_params = [3, 1.0]\n\n# Perform optimization\nresult = minimize(cost_function, initial_params, args=(noisy_image1, noisy_image2),\n                  bounds=[(3, 21), (0.1, 5.0)],  # bounds for kernel size and sigma\n                  method='L-BFGS-B')\n\n# Extract optimized parameters\noptimized_kernel_size, optimized_sigma = result.x\nprint(f\"Optimized kernel size: {int(optimized_kernel_size)}\")\nprint(f\"Optimized sigma: {optimized_sigma}\")\n\n# Apply the optimized denoising function\noptimized_denoised_image = apply_gaussian_filter(noisy_image1, int(optimized_kernel_size), optimized_sigma)\n\n##  Display the Results\n# Display the optimized denoised image\nplt.figure(figsize=(8, 5))\nplt.figure()\nplt.imshow(optimized_denoised_image, cmap='gray')\nplt.title(f'Optimized Denoised Image\\nKernel Size: {int(optimized_kernel_size)}, Sigma: {optimized_sigma:.2f}')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimized kernel size: 3\nOptimized sigma: 1.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 768x480 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-16-output-3.png){width=662 height=419}\n:::\n:::\n\n\n:::{#e153e51b .cell .markdown}\nA from the scratch mathematical implementation of gradient and the gradient descent algorithm is shown below.\n:::\n\n::: {#e21c9db4 .cell execution_count=16}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import mean_squared_error\n\n# Load the clean image\nclean_image = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to ensure kernel size is odd and positive\ndef valid_kernel_size(kernel_size):\n    if kernel_size < 1:\n        kernel_size = 1\n    if kernel_size % 2 == 0:\n        kernel_size += 1\n    return int(kernel_size)\n\n# Function to apply Gaussian smoothing\ndef denoiser(theta, noisy_image):\n    kernel_size = valid_kernel_size(int(theta[0]))  # Ensure the kernel size is an odd integer\n    sigmaX = theta[1]\n    return cv2.GaussianBlur(noisy_image, (kernel_size, kernel_size), sigmaX=sigmaX)\n\n# Function to compute PSNR\ndef compute_psnr(original, denoised):\n    mse = np.mean((original - denoised) ** 2)\n    if mse == 0:\n        return float('inf')\n    psnr = 20 * np.log10(255.0 / np.sqrt(mse))\n    return psnr\n\n# Cost function for scipy optimization\ndef cost_function(theta, noisy_image1, noisy_image2):\n    denoised_image = denoiser(theta, noisy_image1)\n    return np.mean((denoised_image - noisy_image2) ** 2)\n\n# Manual gradient descent for denoising optimization\ndef gradient_descent(noisy_image, noisy_image2, theta, alpha=0.01, iterations=100):\n    for i in range(iterations):\n        # Compute the gradient for each parameter\n        kernel_size, sigmaX = theta\n        kernel_size = valid_kernel_size(kernel_size)\n        \n        # Denoise the image\n        denoised_image = cv2.GaussianBlur(noisy_image, (int(kernel_size), int(kernel_size)), sigmaX)\n\n        # Compute the gradient for both kernel_size and sigmaX\n        grad_kernel = np.sum(2 * (denoised_image - noisy_image2))  # Simplified gradient computation\n        grad_sigma = np.sum(2 * (denoised_image - noisy_image2))   # Simplified gradient computation\n\n        # Update theta using the gradients\n        theta[0] -= alpha * grad_kernel\n        theta[1] -= alpha * grad_sigma\n\n    return theta\n\n# Noisy images\nnoisy_image1 = add_gaussian_noise(clean_image, sigma=20)\nnoisy_image2 = add_gaussian_noise(clean_image, sigma=20)\n\n# Initial parameters (theta): [kernel_size, sigmaX]\ninitial_theta = [5, 1.0]\n\n# Scipy optimizer: Minimize the cost function\nresult = minimize(cost_function, initial_theta, args=(noisy_image1, noisy_image2), method='BFGS', options={'disp': True})\noptimized_theta_scipy = result.x\ndenoised_image_scipy = denoiser(optimized_theta_scipy, noisy_image1)\npsnr_scipy = compute_psnr(clean_image, denoised_image_scipy)\n\n# Manual gradient descent optimizer\noptimized_theta_sgd = gradient_descent(noisy_image1, noisy_image2, initial_theta.copy())\ndenoised_image_sgd = denoiser(optimized_theta_sgd, noisy_image1)\npsnr_sgd = compute_psnr(clean_image, denoised_image_sgd)\npsnr_noisy=compute_psnr(clean_image, noisy_image1)\n# Plot the results\nfig, ax = plt.subplots(1, 4, figsize=(10, 5))\n# Plot 0: Noisy image y\nax[0].imshow(noisy_image1, cmap='gray')\nax[0].set_title(f'Noisy image-1, PSNR: {psnr_noisy:.2f} dB')\nax[0].axis('off')\n\n# Plot 1: Denoised image (scipy optimizer)\nax[1].imshow(denoised_image_scipy, cmap='gray')\nax[1].set_title(f'Scipy Optimizer, PSNR: {psnr_scipy:.2f} dB')\nax[1].axis('off')\n\n# Plot 2: Denoised image (Manual Gradient Descent)\nax[2].imshow(denoised_image_sgd, cmap='gray')\nax[2].set_title(f'Manual Gradient Descent, PSNR: {psnr_sgd:.2f} dB')\nax[2].axis('off')\n\n# Plot 3: Clean image\nax[3].imshow(clean_image, cmap='gray')\nax[3].set_title('Clean Image')\nax[3].axis('off')\nplt.tight_layout()\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 90.614056\n         Iterations: 0\n         Function evaluations: 3\n         Gradient evaluations: 1\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-17-output-2.png){width=964 height=164}\n:::\n:::\n\n\n:::{#6fea1177 .cell .markdown}\nAn alternative approach is to use the Gradient descent algorithm form `sklearn` library.\n:::\n\n::: {#af42aed3 .cell execution_count=17}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDRegressor\n\n# Load the clean image\nclean_image = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Create noisy images\nsigma = 10\nnoisy_image_1 = add_gaussian_noise(clean_image, sigma)\nnoisy_image_2 = add_gaussian_noise(clean_image, sigma)\n\n# Function to apply Gaussian smoothing\ndef apply_gaussian_smoothing(image, ksize, sigmaX):\n    return cv2.GaussianBlur(image, (ksize, ksize), sigmaX=sigmaX)\n\n# Define a custom loss function (Mean Squared Error between two denoised images)\ndef loss_function(ksize, sigmaX, noisy_image_1, noisy_image_2):\n    denoised_image_1 = apply_gaussian_smoothing(noisy_image_1, ksize, sigmaX)\n    denoised_image_2 = apply_gaussian_smoothing(noisy_image_2, ksize, sigmaX)\n    return np.mean((denoised_image_1 - denoised_image_2) ** 2)\n\n# Parameters for optimization\nksize_range = np.array([3, 5, 7, 9])  # Possible kernel sizes\nsigmaX_range = np.array([0.5, 1, 1.5, 2])  # Possible sigmaX values\n\n# Convert the ksize and sigmaX values into feature vectors for SGD\nX_train = np.array([[k, s] for k in ksize_range for s in sigmaX_range])\n\n# Flatten the noisy images for input\ny_train = []\nfor ksize, sigmaX in X_train:\n    loss = loss_function(int(ksize), sigmaX, noisy_image_1, noisy_image_2)\n    y_train.append(loss)\n\n# Training data (input is kernel size and sigmaX, target is the loss value)\ny_train = np.array(y_train)\n\n# Train using SGDRegressor\nsgd = SGDRegressor(max_iter=1000, tol=1e-6)\nsgd.fit(X_train, y_train)\n\n# Predict the optimal kernel size and sigmaX\noptimal_params = sgd.coef_\n\n# Use the optimal parameters to denoise\noptimal_ksize = int(np.clip(optimal_params[0], 3, 9))  # Ensure ksize is odd and within valid range\noptimal_sigmaX = np.clip(optimal_params[1], 0.5, 2)  # Ensure sigmaX is within valid range\n\n# Apply Gaussian smoothing with the optimal parameters\nbest_denoised_image = apply_gaussian_smoothing(noisy_image_1, optimal_ksize, optimal_sigmaX)\n\n# Display the best denoised image\nplt.figure(figsize=(8, 5))\nplt.figure()\nplt.imshow(best_denoised_image, cmap='gray')\nplt.title(f\"Best Denoised Image\\nKsize={optimal_ksize}, SigmaX={optimal_sigmaX}\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 768x480 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-18-output-2.png){width=662 height=419}\n:::\n:::\n\n\n:::{#64e51ce9 .cell .markdown}\nThis attempt is to Compare the quality of denoising with SG descent method.\n:::\n\n::: {#65e0eb88 .cell execution_count=18}\n``` {}\n#| code-overflow: wrap\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Load the clean image\nclean_image = cv2.imread('input.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to compute PSNR\ndef compute_psnr(original, denoised):\n    mse = np.mean((original - denoised) ** 2)\n    if mse == 0:\n        return float('inf')\n    psnr = 20 * np.log10(255.0 / np.sqrt(mse))\n    return psnr\n\n# Noisy images\nnoisy_image1 = add_gaussian_noise(clean_image, sigma=20)\nnoisy_image2 = add_gaussian_noise(clean_image, sigma=20)\n\n# Denoiser function (optimized parameters using sklearn)\ndef denoiser(theta, noisy_image):\n    return cv2.GaussianBlur(noisy_image, (int(theta[0]), int(theta[0])), sigmaX=theta[1])\n\n# Cost function\ndef cost_function(theta, noisy_image1, noisy_image2):\n    denoised_image = denoiser(theta, noisy_image1)\n    return np.mean((denoised_image - noisy_image2) ** 2)\n\n# Gradient Descent optimization using sklearn\ndef optimize_parameters(noisy_image1, noisy_image2, initial_theta):\n    sgd = SGDRegressor(max_iter=1000, tol=1e-3, learning_rate='adaptive')\n    \n    # Reshape theta and noisy_image for fitting\n    noisy_image1_flat = noisy_image1.flatten()\n    noisy_image2_flat = noisy_image2.flatten()\n    \n    X = np.vstack([noisy_image1_flat, noisy_image2_flat]).T\n    y = noisy_image2_flat\n    \n    sgd.fit(X, y)\n    \n    # Optimized parameters (theta)\n    optimized_theta = sgd.coef_\n    \n    return optimized_theta\n\n# Initial theta parameters (manually tuned)\ninitial_theta = [5, 1.0]  # Example: Kernel size 5, sigmaX = 1.0\n\n# Optimize the parameters\noptimized_theta = optimize_parameters(noisy_image1, noisy_image2, initial_theta)\n\n# Apply the denoiser with optimized parameters\ndenoised_image = denoiser(optimized_theta, noisy_image1)\n\n# Compute PSNR between the clean image and the denoised image\npsnr_value = compute_psnr(clean_image, denoised_image)\n\n# Display the PSNR value\nprint(f\"PSNR value between the clean image and the denoised image: {psnr_value:.2f} dB\")\n\n# Display the denoised image\nplt.figure(figsize=(8, 5))\nplt.imshow(denoised_image, cmap='gray')\nplt.title(f\"Denoised Image (PSNR: {psnr_value:.2f} dB)\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPSNR value between the clean image and the denoised image: 31.44 dB\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](review_files/figure-html/cell-19-output-2.png){width=758 height=455}\n:::\n:::\n\n\n:::{#fe715b16 .cell .markdown}\nInstead of using the default optimizer, let's try the finite difference approach for numerical differentiation. This approach is purely depended on the function values rather than its mathematical definition. So, computationally it will be more dependable. Improved code with fine tuning gradient descent with finite difference is shown below.\n:::\n\n::: {#cell-fig-denoising-plot .cell execution_count=19}\n``` {}\n#| label: fig-denoising-plot\n#| fig-cap: Example of denoising results.\n#| fig-alt: A visualization of parameter fine tuning.\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nclean_image = cv2.imread('/manuscript-template-vscode-main/images/first.png', cv2.IMREAD_GRAYSCALE)\n\n# Function to add Gaussian noise\ndef add_gaussian_noise(image, sigma):\n    row, col = image.shape\n    mean = 0\n    gauss = np.random.normal(mean, sigma, (row, col))\n    noisy_image = image + gauss\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to [0, 255]\n    return noisy_image.astype(np.uint8)\n\n# Function to apply Gaussian smoothing\ndef apply_gaussian_smoothing(noisy_image, ksize, sigmaX):\n    ksize = int(ksize)\n    if ksize % 2 == 0:\n        ksize += 1\n    return cv2.GaussianBlur(noisy_image, (ksize, ksize), sigmaX=sigmaX)\n\n# Function to ensure valid kernel size (odd and positive)\ndef valid_kernel_size(size):\n    size = int(size)\n    if size % 2 == 0:\n        size += 1\n    return max(size, 1)  # Ensure size is at least 1\n\n# Function to compute the cost\ndef cost_function(theta, noisy_image1, noisy_image2):\n    kernel_size, sigmaX = theta\n    kernel_size = valid_kernel_size(kernel_size)\n    denoised_image = apply_gaussian_smoothing(noisy_image1, kernel_size, sigmaX)\n    residual = denoised_image - noisy_image2\n    return np.sum(residual ** 2)\n\n# Gradient descent optimizer\ndef gradient_descent(noisy_image1, noisy_image2, initial_theta, alpha=0.01, iterations=100):\n    theta = np.array(initial_theta)\n    for _ in range(iterations):\n        grad_kernel, grad_sigma = compute_gradients(noisy_image1, noisy_image2, theta)\n        theta[0] -= alpha * grad_kernel\n        theta[1] -= alpha * grad_sigma\n        theta[0] = valid_kernel_size(theta[0])\n    return theta\n\n# Compute numerical gradients\ndef compute_gradients(noisy_image1, noisy_image2, theta):\n    epsilon = 1e-5\n    cost_1 = cost_function([theta[0] + epsilon, theta[1]], noisy_image1, noisy_image2)\n    cost_2 = cost_function([theta[0] - epsilon, theta[1]], noisy_image1, noisy_image2)\n    grad_kernel = (cost_1 - cost_2) / (2 * epsilon)\n    \n    cost_1 = cost_function([theta[0], theta[1] + epsilon], noisy_image1, noisy_image2)\n    cost_2 = cost_function([theta[0], theta[1] - epsilon], noisy_image1, noisy_image2)\n    grad_sigma = (cost_1 - cost_2) / (2 * epsilon)\n    \n    return grad_kernel, grad_sigma\n\n# Parameters for noisy images\nsigma_values = [10, 15]  # Different levels of Gaussian noise\n\n# Generate noisy images\nnoisy_images = [add_gaussian_noise(clean_image, sigma) for sigma in sigma_values]\n\n# Using scipy's minimize function\ninitial_theta = [15, 1]  # Initial guess for kernel size and sigmaX\nresult = minimize(cost_function, initial_theta, args=(noisy_images[0], noisy_images[1]), method='BFGS')\noptimized_theta_scipy = result.x\noptimized_denoised_image_scipy = apply_gaussian_smoothing(noisy_images[0], valid_kernel_size(optimized_theta_scipy[0]), optimized_theta_scipy[1])\n\n# Using manual gradient descent optimization\noptimized_theta_gd = gradient_descent(noisy_images[0], noisy_images[1], initial_theta, alpha=0.01, iterations=100)\noptimized_denoised_image_gd = apply_gaussian_smoothing(noisy_images[0], valid_kernel_size(optimized_theta_gd[0]), optimized_theta_gd[1])\n\n# Compute PSNR values\ndef compute_psnr(image1, image2):\n    mse = np.mean((image1 - image2) ** 2)\n    max_pixel = 255.0\n    if mse == 0:\n        return 100\n    return 20 * np.log10(max_pixel / np.sqrt(mse))\n\npsnr_scipy = compute_psnr(optimized_denoised_image_scipy, clean_image)\npsnr_gd = compute_psnr(optimized_denoised_image_gd, clean_image)\npsnr_noisy=compute_psnr(noisy_images[0], clean_image)\n# Display results\nplt.figure(figsize=(8, 5))\n\n# Plot denoised image by scipy optimizer\nplt.subplot(2, 2, 1)\nplt.imshow(noisy_images[0], cmap='gray')\nplt.title(f'Noisy image-1\\nPSNR: {psnr_noisy:.2f}')\nplt.axis('off')\n# Plot denoised image by scipy optimizer\nplt.subplot(2, 2, 2)\nplt.imshow(optimized_denoised_image_scipy, cmap='gray')\nplt.title(f'Scipy Optimizer\\nPSNR: {psnr_scipy:.2f}')\nplt.axis('off')\n# Plot denoised image by manual gradient descent\nplt.subplot(2, 2, 3)\nplt.imshow(optimized_denoised_image_gd, cmap='gray')\nplt.title(f'Gradient Descent\\nPSNR: {psnr_gd:.2f}')\nplt.axis('off')\n\n# Plot clean image\nplt.subplot(2, 2, 4)\nplt.imshow(clean_image, cmap='gray')\nplt.title('Clean Image')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Example of denoising results.](review_files/figure-html/fig-denoising-plot-output-1.png){#fig-denoising-plot width=526 height=471 fig-alt='A visualization of parameter fine tuning.'}\n:::\n:::\n\n\n:::{#5ca54325 .cell .markdown}\n:::{.callout-warning}\n## A Missing element\n\nIn all these fine-tuning implementations, we started with a initial $\\theta$, which is neither relevant to the nosy images nor the clean image. So, the accuracy of the estimated parameters may not be realistic. A solution to this drawback in the fine tuning is given in the article. *... For initialization, we use a fixed $\\theta_0$, found by manually tuning $\\theta$ for a single image from the BS400 dataset*. \n\n:::\n:::\n\n",
    "supporting": [
      "review_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}